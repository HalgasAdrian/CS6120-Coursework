{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1: Natural Language Processing**\n",
        "\n",
        "Movie Review Sentiment Analysis\n",
        "\n",
        "The goal of this assignment is to implement and compare three text classification algorithms—\n",
        "Naive Bayes, Logistic Regression, and Multilayer Perceptron (MLP)—on the NLTK Movie Reviews\n",
        "dataset. You will explore the impact of using both raw Term Frequency (TF) and Term Frequency-\n",
        "Inverse Document Frequency (TF-IDF) as feature representations."
      ],
      "metadata": {
        "id": "Rfz6ZITMyBiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Preparation**\n",
        "- Load the NLTK Movie Reviews Dataset"
      ],
      "metadata": {
        "id": "4xwI1sQoyro0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aP7Ivk4Hw3Nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7e44c0-0356-4407-ce3d-d3903e78b70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "# Download the IMDb movie reviews dataset\n",
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Access the Dataset**\n",
        "- Once downloaded, we acess the data using the following code"
      ],
      "metadata": {
        "id": "Qz-W384szg2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "# Access the movie reviews and labels\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "for category in movie_reviews.categories()\n",
        "for fileid in movie_reviews.fileids(category)]\n",
        "# Shuffle the documents to ensure a balanced distribution of positive and negative reviews\n",
        "import random\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "LqeOVg9RzsP1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the Dataset**\n",
        "- Take a look at the structure of the dataset and sample reviews to understand its characteristics\n"
      ],
      "metadata": {
        "id": "HvPZDUUcz3A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first review and its label\n",
        "print(\"Sample Review:\", documents[0][0][:10]) # Displaying the first 10 words for brevity\n",
        "print(\"Label:\", documents[0][1])"
      ],
      "metadata": {
        "id": "VTsZkjNFz-tS",
        "outputId": "c4038d61-e2aa-4d89-b6dc-564a4032d2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Review: ['talk', 'about', 'a', 'movie', 'that', 'seemed', 'dated', 'before', 'it', 'even']\n",
            "Label: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the dataset by tokenization (use nltk punkt tokenizer), stemming/lemmatization, and remove stopwords.\n",
        "\n",
        "- I will use lemmatization here"
      ],
      "metadata": {
        "id": "O1Ra5jzK16Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load and preprocess the data\n",
        "documents = []\n",
        "for category in movie_reviews.categories():\n",
        "    for fileid in movie_reviews.fileids(category):\n",
        "        raw_words = movie_reviews.raw(fileid)\n",
        "        tokens = word_tokenize(raw_words)  # Tokenize using Punkt tokenizer\n",
        "        tokens = [word.lower() for word in tokens if word.isalpha()]  # Lowercase and remove punctuation\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatize and remove stopwords\n",
        "        documents.append((tokens, category))\n",
        "\n",
        "# Shuffle the documents\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Display a sample\n",
        "print(\"Sample Review:\", documents[0][0][:10])\n",
        "print(\"Label:\", documents[0][1])"
      ],
      "metadata": {
        "id": "ab6SwZUf2D97",
        "outputId": "cda6722b-299b-46d5-a142-541dbe0f7edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Review: ['bob', 'happy', 'bastard', 'quickie', 'review', 'rush', 'hour', 'problem', 'hour', 'clone']\n",
            "Label: pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Coverage Analysis Insights**\n",
        "\n",
        "- Conduct a coverage analysis to identify the percentage of unique words covered by the\n",
        "preprocessing steps.\n",
        "\n",
        "- Visualize the coverage analysis with the y-axis representing coverage percentage and the x-\n",
        "axis representing the id of tokens (words) ordered by frequency of occurrence. Use a line plot\n",
        "for clarity.\n",
        "\n",
        "- Discuss the insights gained from the coverage analysis. Consider questions such as:\n",
        "\n",
        " - How does the coverage change with the number of tokens considered?\n",
        "\n",
        " - At what point does the coverage seem to stabilize?\n",
        "\n",
        " - Are there diminishing returns in terms of coverage as the number of tokens increases?"
      ],
      "metadata": {
        "id": "8-LUEk6f2-1W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81atDLnG3QTd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}